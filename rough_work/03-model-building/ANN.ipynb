{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import data cleaning libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Import machine learning libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import data visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import warning libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set working directory\n",
    "# Set this to your own path\n",
    "os.chdir('/home/shaw/Documents/GitHub/crop-yield-estimate/')\n",
    "# Set this to your own path\n",
    "sys.path.insert(0, '/home/shaw/Documents/GitHub/crop-yield-estimate/pipeline')\n",
    "\n",
    "# Import preprocessing libraries\n",
    "# Import system libraries\n",
    "from preprocessing import dim_reduction\n",
    "from preprocessing import feature_selection\n",
    "from preprocessing import scaling\n",
    "from preprocessing import feature_engineering\n",
    "from preprocessing import cleaning\n",
    "# Preprocess data\n",
    "train_path = \"data/Train.csv\"\n",
    "test_path = \"data/Test.csv\"\n",
    "df = cleaning.clean_data(train_path, test_path)\n",
    "df = feature_engineering.get_features(df)\n",
    "df = scaling.scale_features(df)\n",
    "df = feature_selection.select_features(df)\n",
    "df = dim_reduction.reduce_dim(df)\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "df_train = df[df['Yield'].isna() == False]\n",
    "df_test = df[df['Yield'].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(axis=1, inplace=True)\n",
    "df_test.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.astype('float32')\n",
    "df_test = df_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X, y = df_train.drop([\"Yield\", \"Yield_per_Acre\"], axis=1), df_train[\"Yield\"]\n",
    "\n",
    "X_data = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_data = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: torch.Size([3096, 178])\n",
      "X_test size: torch.Size([774, 178])\n",
      "y_train size: torch.Size([3096])\n",
      "y_test size: torch.Size([774])\n"
     ]
    }
   ],
   "source": [
    "# Verify the sizes of the data\n",
    "print(\"X_train size:\", X_train.shape)\n",
    "print(\"X_test size:\", X_test.shape)\n",
    "print(\"y_train size:\", y_train.shape)\n",
    "print(\"y_test size:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropYeild(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(178, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 96)\n",
    "        self.fc4 = nn.Linear(96, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.34)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CropYeild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.008 # Idk random number - i'LL experiement\n",
    "criteria = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred size: torch.Size([3870, 1])\n",
      "y_data size: torch.Size([3870])\n"
     ]
    }
   ],
   "source": [
    "# Forwards pass\n",
    "y_pred = model(X_data)\n",
    "\n",
    "print(\"y_pred size:\", y_pred.shape)\n",
    "print(\"y_data size:\", y_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[ 1.2383e+01],\n",
      "        [ 3.2352e+00],\n",
      "        [ 2.1234e+00],\n",
      "        [-1.4184e+00],\n",
      "        [ 9.4228e-01],\n",
      "        [ 1.7749e+00],\n",
      "        [ 3.4008e+00],\n",
      "        [ 5.9613e+00],\n",
      "        [-3.1399e+00],\n",
      "        [ 1.0380e+01],\n",
      "        [ 5.2092e+00],\n",
      "        [-3.1026e+00],\n",
      "        [ 5.3408e+00],\n",
      "        [-4.1729e+00],\n",
      "        [ 5.7953e+00],\n",
      "        [ 1.0463e+01],\n",
      "        [ 2.7130e+00],\n",
      "        [ 5.0424e+00],\n",
      "        [ 6.8812e-01],\n",
      "        [ 5.6233e+00],\n",
      "        [-5.0125e-01],\n",
      "        [ 1.0304e+00],\n",
      "        [ 6.6252e+00],\n",
      "        [-8.7595e-01],\n",
      "        [ 1.0978e-02],\n",
      "        [-1.5299e+00],\n",
      "        [ 1.3786e+00],\n",
      "        [ 3.4024e+00],\n",
      "        [ 5.1611e+00],\n",
      "        [ 6.6624e+00],\n",
      "        [ 1.1139e+00],\n",
      "        [ 8.1915e+00],\n",
      "        [ 2.0676e-01],\n",
      "        [-2.7609e+00],\n",
      "        [ 1.4617e+00],\n",
      "        [ 4.1796e+00],\n",
      "        [-1.9105e+00],\n",
      "        [ 4.4911e+00],\n",
      "        [ 6.3836e-01],\n",
      "        [ 3.9360e+00],\n",
      "        [-3.1680e+00],\n",
      "        [ 5.1237e+00],\n",
      "        [ 1.0770e+01],\n",
      "        [-7.8535e-01],\n",
      "        [ 1.0084e+01],\n",
      "        [ 1.3045e+00],\n",
      "        [ 4.1875e-01],\n",
      "        [ 5.4523e+00],\n",
      "        [ 2.8459e+00],\n",
      "        [ 3.8030e+00]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('pred:', y_pred[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 | Train Loss: 764.90906, | Test Loss: 1072.25647\n",
      "Epoch:   20 | Train Loss: 412.22363, | Test Loss: 804.39618\n",
      "Epoch:   40 | Train Loss: 375.18805, | Test Loss: 805.98010\n",
      "Epoch:   60 | Train Loss: 371.41504, | Test Loss: 807.50635\n",
      "Epoch:   80 | Train Loss: 366.87921, | Test Loss: 815.73602\n",
      "Epoch:  100 | Train Loss: 367.99585, | Test Loss: 819.67157\n",
      "Epoch:  120 | Train Loss: 364.01865, | Test Loss: 817.71521\n",
      "Epoch:  140 | Train Loss: 356.12604, | Test Loss: 808.17993\n",
      "Epoch:  160 | Train Loss: 367.42111, | Test Loss: 836.49323\n",
      "Epoch:  180 | Train Loss: 360.25235, | Test Loss: 830.50262\n",
      "Epoch:  200 | Train Loss: 353.94003, | Test Loss: 829.94159\n",
      "Epoch:  220 | Train Loss: 352.80417, | Test Loss: 830.16235\n",
      "Epoch:  240 | Train Loss: 347.90887, | Test Loss: 827.15332\n",
      "Epoch:  260 | Train Loss: 350.21796, | Test Loss: 845.01276\n",
      "Epoch:  280 | Train Loss: 352.09573, | Test Loss: 827.91931\n",
      "Epoch:  300 | Train Loss: 335.41864, | Test Loss: 829.99115\n",
      "Epoch:  320 | Train Loss: 346.82178, | Test Loss: 842.05920\n",
      "Epoch:  340 | Train Loss: 350.25195, | Test Loss: 840.72241\n",
      "Epoch:  360 | Train Loss: 347.04666, | Test Loss: 826.52594\n",
      "Epoch:  380 | Train Loss: 336.22415, | Test Loss: 832.64368\n",
      "Epoch:  400 | Train Loss: 347.09631, | Test Loss: 834.14929\n",
      "Epoch:  420 | Train Loss: 332.31393, | Test Loss: 841.46149\n",
      "Epoch:  440 | Train Loss: 339.67743, | Test Loss: 825.95288\n",
      "Epoch:  460 | Train Loss: 332.21332, | Test Loss: 840.23737\n",
      "Epoch:  480 | Train Loss: 336.68439, | Test Loss: 824.51422\n",
      "Epoch:  500 | Train Loss: 323.56094, | Test Loss: 845.74634\n",
      "Epoch:  520 | Train Loss: 334.11777, | Test Loss: 855.85284\n",
      "Epoch:  540 | Train Loss: 326.11319, | Test Loss: 845.64032\n",
      "Epoch:  560 | Train Loss: 333.04306, | Test Loss: 839.17334\n",
      "Epoch:  580 | Train Loss: 336.14209, | Test Loss: 863.58228\n",
      "Epoch:  600 | Train Loss: 335.21463, | Test Loss: 861.63721\n",
      "Epoch:  620 | Train Loss: 331.39655, | Test Loss: 843.53766\n",
      "Epoch:  640 | Train Loss: 323.61877, | Test Loss: 869.18542\n",
      "Epoch:  660 | Train Loss: 337.47791, | Test Loss: 863.47626\n",
      "Epoch:  680 | Train Loss: 329.90271, | Test Loss: 852.94238\n",
      "Epoch:  700 | Train Loss: 331.71674, | Test Loss: 872.00513\n",
      "Epoch:  720 | Train Loss: 343.23682, | Test Loss: 854.96167\n",
      "Epoch:  740 | Train Loss: 353.30219, | Test Loss: 871.48267\n",
      "Epoch:  760 | Train Loss: 321.37036, | Test Loss: 854.67444\n",
      "Epoch:  780 | Train Loss: 329.19281, | Test Loss: 862.55499\n",
      "Epoch:  800 | Train Loss: 328.57388, | Test Loss: 891.41040\n",
      "Epoch:  820 | Train Loss: 325.06726, | Test Loss: 890.93964\n",
      "Epoch:  840 | Train Loss: 316.42752, | Test Loss: 882.03882\n",
      "Epoch:  860 | Train Loss: 334.34564, | Test Loss: 914.08508\n",
      "Epoch:  880 | Train Loss: 330.48849, | Test Loss: 897.43518\n",
      "Epoch:  900 | Train Loss: 345.54800, | Test Loss: 905.22455\n",
      "Epoch:  920 | Train Loss: 342.57855, | Test Loss: 954.30212\n",
      "Epoch:  940 | Train Loss: 332.22311, | Test Loss: 950.05359\n",
      "Epoch:  960 | Train Loss: 340.18121, | Test Loss: 963.02997\n",
      "Epoch:  980 | Train Loss: 335.08109, | Test Loss: 960.10242\n",
      "Epoch: 1000 | Train Loss: 341.70084, | Test Loss: 981.96674\n",
      "Epoch: 1020 | Train Loss: 342.79123, | Test Loss: 1030.52783\n",
      "Epoch: 1040 | Train Loss: 352.42328, | Test Loss: 1000.47198\n",
      "Epoch: 1060 | Train Loss: 339.74588, | Test Loss: 987.44452\n",
      "Epoch: 1080 | Train Loss: 354.78928, | Test Loss: 975.47540\n",
      "Epoch: 1100 | Train Loss: 326.83905, | Test Loss: 999.10052\n",
      "Epoch: 1120 | Train Loss: 338.80197, | Test Loss: 1041.67859\n",
      "Epoch: 1140 | Train Loss: 323.52835, | Test Loss: 1077.69104\n",
      "Epoch: 1160 | Train Loss: 335.65491, | Test Loss: 1074.01599\n",
      "Epoch: 1180 | Train Loss: 327.61362, | Test Loss: 1095.32544\n",
      "Epoch: 1200 | Train Loss: 331.78073, | Test Loss: 1120.09521\n",
      "Epoch: 1220 | Train Loss: 330.25195, | Test Loss: 1140.33777\n",
      "Epoch: 1240 | Train Loss: 339.37637, | Test Loss: 1131.48022\n",
      "Epoch: 1260 | Train Loss: 328.87729, | Test Loss: 1153.36365\n",
      "Epoch: 1280 | Train Loss: 324.35287, | Test Loss: 1127.58594\n",
      "Epoch: 1300 | Train Loss: 342.01025, | Test Loss: 1138.63611\n",
      "Epoch: 1320 | Train Loss: 325.07059, | Test Loss: 1134.53772\n",
      "Epoch: 1340 | Train Loss: 333.49249, | Test Loss: 1114.93237\n",
      "Epoch: 1360 | Train Loss: 335.79718, | Test Loss: 1125.94641\n",
      "Epoch: 1380 | Train Loss: 329.18643, | Test Loss: 1164.52344\n",
      "Epoch: 1400 | Train Loss: 315.70862, | Test Loss: 1193.29138\n",
      "Epoch: 1420 | Train Loss: 339.87439, | Test Loss: 1186.24426\n",
      "Epoch: 1440 | Train Loss: 339.32755, | Test Loss: 1203.94604\n",
      "Epoch: 1460 | Train Loss: 341.97217, | Test Loss: 1164.43628\n",
      "Epoch: 1480 | Train Loss: 322.79886, | Test Loss: 1159.62207\n",
      "Epoch: 1500 | Train Loss: 307.89667, | Test Loss: 1160.63232\n",
      "Epoch: 1520 | Train Loss: 315.43948, | Test Loss: 1150.30444\n",
      "Epoch: 1540 | Train Loss: 328.16367, | Test Loss: 1135.77722\n",
      "Epoch: 1560 | Train Loss: 325.85568, | Test Loss: 1099.72644\n",
      "Epoch: 1580 | Train Loss: 320.88321, | Test Loss: 1083.35852\n",
      "Epoch: 1600 | Train Loss: 319.26810, | Test Loss: 1007.54840\n",
      "Epoch: 1620 | Train Loss: 308.39590, | Test Loss: 1017.47314\n",
      "Epoch: 1640 | Train Loss: 311.52161, | Test Loss: 1005.51385\n",
      "Epoch: 1660 | Train Loss: 331.45407, | Test Loss: 1004.54468\n",
      "Epoch: 1680 | Train Loss: 318.69913, | Test Loss: 1075.69678\n",
      "Epoch: 1700 | Train Loss: 334.60464, | Test Loss: 1111.79895\n",
      "Epoch: 1720 | Train Loss: 338.94183, | Test Loss: 1050.91467\n",
      "Epoch: 1740 | Train Loss: 322.73788, | Test Loss: 1098.69739\n",
      "Epoch: 1760 | Train Loss: 322.42062, | Test Loss: 1158.06604\n",
      "Epoch: 1780 | Train Loss: 327.81000, | Test Loss: 1091.70605\n",
      "Epoch: 1800 | Train Loss: 334.92142, | Test Loss: 1125.13513\n",
      "Epoch: 1820 | Train Loss: 323.32266, | Test Loss: 1160.24353\n",
      "Epoch: 1840 | Train Loss: 307.28342, | Test Loss: 1098.33630\n",
      "Epoch: 1860 | Train Loss: 333.02597, | Test Loss: 1108.55530\n",
      "Epoch: 1880 | Train Loss: 319.80762, | Test Loss: 1117.70764\n",
      "Epoch: 1900 | Train Loss: 343.64810, | Test Loss: 1125.55322\n",
      "Epoch: 1920 | Train Loss: 298.56940, | Test Loss: 1181.48804\n",
      "Epoch: 1940 | Train Loss: 332.99432, | Test Loss: 1149.95618\n",
      "Epoch: 1960 | Train Loss: 329.38556, | Test Loss: 1197.37000\n",
      "Epoch: 1980 | Train Loss: 311.76465, | Test Loss: 1220.94861\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "epoch_count, train_loss, test_loss = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model(X_train).squeeze()\n",
    "\n",
    "    loss_value = torch.sqrt(criteria(y_pred, y_train))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_value.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        y_pred_test = model(X_test).squeeze()\n",
    "\n",
    "        loss_value_test = torch.sqrt(criteria(y_pred_test, y_test))\n",
    "\n",
    "    train_loss.append(loss_value.item())\n",
    "\n",
    "    if epoch % int(epochs / 100) == 0:\n",
    "        print(f'Epoch: {epoch:4.0f} | Train Loss: {loss_value:.5f}, | Test Loss: {loss_value_test:.5f}')\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss.append(loss_value.detach().numpy())\n",
    "        test_loss.append(loss_value_test.detach().numpy())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
